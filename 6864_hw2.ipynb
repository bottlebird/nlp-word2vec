{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"6864_hw2.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"FU7xWiY6TyWS","colab_type":"code","colab":{}},"source":["%%bash\n","!(stat -t /usr/local/lib/*/dist-packages/google/colab > /dev/null 2>&1) && exit \n","rm -rf 6864-hw2\n","git clone https://github.com/lingo-mit/6864-hw2.git"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"A0MHaHrdUACZ","colab_type":"code","colab":{}},"source":["import sys\n","sys.path.append(\"/content/6864-hw2\")\n","\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EyHSnyLuhOYS","colab_type":"text"},"source":["# **Introduction**"]},{"cell_type":"markdown","metadata":{"id":"lsSBIE91TvsS","colab_type":"text"},"source":["In this lab, you will explore two types of recurrent architectures to perform sequential classification. Specifically, you are asked to implement and train a vanilla RNN (no gate mechanism) and an LSTM for the task of Name Entity Recognition (NER). For details about NER, you can read [this](https://en.wikipedia.org/wiki/Named-entity_recognition) wikipage.\n","\n","To complete this lab, you need to first understand the recurrent update equations introduced in lecture, and transform them into PyTorch code.\n","\n","In NER, your task is to predict the correct name entity of each word in a sentence.\n","We are going to work with a dataset called MIT-Restaurants. We've helped you preprocessed the data. They now look like this:"]},{"cell_type":"code","metadata":{"id":"ZKNZ5OI2Cztq","colab_type":"code","colab":{}},"source":["def read_file(f_name):\n","    data = []\n","    with open(f_name, 'r') as f:\n","        for line in f:\n","            data.append(line.strip().split())\n","    return data\n","\n","train_data = read_file('/content/6864-hw2/train.dat')\n","train_tags = read_file('/content/6864-hw2/train.tag')\n","\n","test_data = read_file('/content/6864-hw2/test.dat')\n","test_tags = read_file('/content/6864-hw2/test.tag')\n","\n","print('Total amount of training samples: %d' % len(train_data))\n","print('Total amount of testing samples: %d' % len(test_data))\n","print('Average sentence length in training data: %f' % (\n","    np.mean([len(sent) for sent in train_data])))\n","\n","print('\\nExample:')\n","print('The first sentence is: ' + str(train_data[0]))\n","print('Its corresponding name entity sequence is: ' + str(train_tags[0]))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qk6wCHKWTtmC","colab_type":"text"},"source":["Here are some helper functions and more data preprocessing before we move on to implementing our models. No code to write in this section but please do understand it as some variables defined here will be used later for training and evaluation. "]},{"cell_type":"code","metadata":{"id":"w_RRJxRJmOGv","colab_type":"code","colab":{}},"source":["# From train data, collect all unique word types as a set and add 'UNK' to it.\n","# Unseen words in test data will be turned into 'UNK'.\n","vocab_set = list(set([word for sent in train_data for word in sent])) + ['UNK']\n","num_vocabs = len(vocab_set)\n","print(\"Number of word types, including 'UNK': %d\" % num_vocabs)\n","\n","# Assign each word type an unique id.\n","vocab2id = {v : i for i, v in enumerate(vocab_set)}\n","\n","\n","# We also collect all tag (class) types and assign an unique id to each of them.\n","# There won't be unseen tag type in test data.\n","tag_set = list(set([tag for tag_seq in train_tags for tag in tag_seq]))\n","num_tags = len(tag_set)\n","print(\"Number of tag types: %d\" % num_tags)\n","print('These are the tag types: ' + str(tag_set))\n","\n","tag2id = {t : i for i, t in enumerate(tag_set)}\n","# Inverse dictionary of tag2id, you will need this during evaluation!\n","id2tag = {i : t for t, i in tag2id.items()}"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Co90EmqjxABh","colab_type":"text"},"source":["Now that you have understood what raw data look like, it's your turn to transform them into the format that our model actually takes as input: one-hot encoding. Hint: in tag_to_id function, the B-I-O refers to beginning, inside and outside. Each of them means the beginning of the tagging chunk, inside the tagging chunk and outside the tagging chunk. For more detailed explanation, please refer to the IOB2 format in [this](https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging)) wikipage."]},{"cell_type":"code","metadata":{"id":"1kPRWdKkwdnf","colab_type":"code","colab":{}},"source":["def one_hot_encoding(sent, vocab2id):\n","    # Example input `sent` (a list of words):\n","    # ['2', 'start', 'restaurants', 'with', 'inside', 'dining']\n","\n","    one_hot = torch.zeros(len(sent), len(vocab2id))\n","    # Your code here!\n","\n","    return one_hot\n","\n","\n","def tag_to_id(tag_seq, tag2id):\n","    # Example input `tag_seq` (a list of tags):\n","    # ['B-Rating', 'I-Rating', 'O', 'O', 'B-Amenity', 'I-Amenity']\n","\n","    id_seq = torch.zeros(len(tag_seq), dtype=torch.long)\n","    # Your code here!\n","\n","    return id_seq\n","\n","\n","# Apply one-hot encoding to data.\n","train_data_oh_list = [one_hot_encoding(sent, vocab2id) for sent in train_data]\n","# Transform tag names into ids.\n","train_tags_id_list = [tag_to_id(tag_seq, tag2id) for tag_seq in train_tags]\n","\n","# train_data_oh_list should now be a list of 2d-tensors, each has shape (sent_len, num_vocabs)\n","# Note that to utilize the `shape` attribute, each element in the list should\n","# already be a torch tensor.\n","print(\"First sentence has shape: %s\" % str(train_data_oh_list[0].shape))\n","print(\"Fifth sentence has shape: %s\" % str(train_data_oh_list[4].shape))\n","# train_tags_id_list is a list of 1d-tensors, each that has shape (sent_len,)\n","print(\"first tag sequence has shape: %s\" % train_tags_id_list[0].shape)\n","print(\"Fifth tag sequence has shape: %s\" % train_tags_id_list[4].shape)\n","\n","\n","# Apply same conversion to test dataset.\n","test_data_oh_list = [one_hot_encoding(sent, vocab2id) for sent in test_data]\n","test_tags_id_list = [tag_to_id(tag_seq, tag2id) for tag_seq in test_tags]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"64AfhgX35gL3","colab_type":"text"},"source":["# **Part 1: Recurrent Neural Network (RNN)**\n","\n","In this part, you will implement a vanilla RNN. You are not allowed to use the PyTorch built-in RNN/RNNCell modules--you will have to implement the update rules yourself. Please follow the formulation introduced in lecture."]},{"cell_type":"code","metadata":{"id":"l7j9a_UYLfTl","colab_type":"code","colab":{}},"source":["class RNN(nn.Module):\n","    # A torch module implementing an RNN. The `forward` function should just\n","    # perform one step of update and output logits before softmax.\n","\n","    def __init__(self, input_size, hidden_size, output_size):\n","        super(RNN, self).__init__()\n","        # `input_size`, `hidden_size`, and `output_size` are all int.\n","\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.output_size = output_size\n","\n","        # Your code here!\n","\n","\n","    def forward(self, input, hidden):\n","        # `input` is a 2d-tensor of shape (1, input_size); `hidden` is another\n","        # 2d-tensor of shape (1, hidden_size), representing the hidden state of\n","        # the previous time step.\n","\n","        output = torch.zeros(1, self.output_size)\n","        # Your code here!\n","\n","        return output, hidden\n","\n","    def initHidden(self):\n","        # Use to initialize hidden state everytime before running a sentence.\n","        return torch.zeros(1, self.hidden_size)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9O9r0TTw9A7Y","colab_type":"text"},"source":["Now that you have defined your RNN model, we can start training it. We've provided the main training loop, but you will have to implement the fucntion `rnn_train_one_sample`, which takes a (sentence-tensor, tag-tensor)-pair as input and does one step of gradient update. To understand better what this function is supposed to do, you can go over the main training loop in the next section first."]},{"cell_type":"code","metadata":{"id":"HF0KIrIIkp9u","colab_type":"code","colab":{}},"source":["learning_rate = 1e-3\n","rnn_hidden_size = 128\n","\n","rnn_model = RNN(input_size=num_vocabs, hidden_size=rnn_hidden_size,\n","                output_size=num_tags).to(device)\n","\n","criterion = nn.CrossEntropyLoss()\n","rnn_optimizer = torch.optim.Adam(rnn_model.parameters(), lr=learning_rate)\n","\n","\n","def rnn_train_one_sample(model, sent_tensor, tag_tensor):\n","    # Run through a sentence, generate output, compute loss, and perform one\n","    # gradient update. Sentence and tag are represented as a 2d-tensor\n","    # `sent_tensor` and a 1d-tensor `tag_tensor`, respectively.\n","\n","    # Initialize hidden state.\n","    hidden = model.initHidden().to(device)\n","\n","    # Your code here!\n","\n","\n","    loss = criterion(outputs, tag_tensor)\n","\n","    rnn_optimizer.zero_grad()\n","    loss.backward()\n","    rnn_optimizer.step()\n","\n","    return outputs, loss.item()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D8o1dpsnBymS","colab_type":"text"},"source":["Here's the main training loop for training RNN:"]},{"cell_type":"code","metadata":{"id":"1OuwfUHyrWzC","colab_type":"code","colab":{}},"source":["import time\n","import math\n","\n","\n","n_epochs = 5\n","iter_count = 0\n","print_every = 1000\n","plot_every = 50\n","\n","# Keep track of losses for plotting\n","current_loss = 0\n","all_losses = []\n","\n","def timeSince(since):\n","    now = time.time()\n","    s = now - since\n","    m = math.floor(s / 60)\n","    s -= m * 60\n","    return '%dm %ds' % (m, s)\n","\n","start = time.time()\n","\n","rnn_model.train()\n","for epoch_i in range(n_epochs):\n","    for sent_tensor, tag_tensor in zip(train_data_oh_list, train_tags_id_list):\n","        sent_tensor = sent_tensor.to(device)\n","        tag_tensor = tag_tensor.to(device)\n","  \n","        output, loss = rnn_train_one_sample(rnn_model, sent_tensor, tag_tensor)\n","        current_loss += loss\n","\n","        if iter_count % print_every == 0:\n","            print('%d %s %.4f' % (iter_count, timeSince(start), loss))\n","\n","        # Add current loss avg to list of losses\n","        if iter_count % plot_every == 0 and iter_count > 0:\n","            all_losses.append(current_loss / plot_every)\n","            current_loss = 0\n","\n","        iter_count += 1"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bzGil8wxCS8X","colab_type":"text"},"source":["Now let's plot the learning curve. The x-axis is the training iterations and the y-axis is the training loss. The loss should be going down."]},{"cell_type":"code","metadata":{"id":"UY0oyFUeLuEu","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker\n","\n","plt.figure()\n","plt.plot(all_losses)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0Q8Hxy-7C8xB","colab_type":"text"},"source":["Now that we have trained our RNN, it's time to evaluate it on the test set."]},{"cell_type":"code","metadata":{"id":"X6R9rSlTlLTf","colab_type":"code","colab":{}},"source":["# Evaluation\n","\n","import sklearn\n","from sklearn.metrics import precision_recall_fscore_support\n","\n","\n","def evaluate_result(true_tag_list, predicted_tag_list):\n","    # Example: true_tag_list/predicted_tag_list:\n","    #   [[‘O’, ‘O’, ‘I’, ‘N’, ...]\n","    #    [‘I’, ‘I’, ‘O’, ‘N’, ...]],\n","    # each sublist corresponds to an input sentence.\n","    p_list = []\n","    r_list = []\n","    f1_list = []\n","    for true_tag, predicted_tag in zip(true_tag_list, predicted_tag_list):\n","        p, r, f1, _ = precision_recall_fscore_support(true_tag, predicted_tag,\n","                                                      average='macro',\n","                                                      zero_division=0)\n","        p_list.append(p)\n","        r_list.append(r)\n","        f1_list.append(f1)\n","    return np.mean(p_list), np.mean(r_list), np.mean(f1_list)\n","\n","\n","# Make prediction for one sentence.\n","def rnn_predict_one_sent(model, sent_tensor):\n","    hidden = model.initHidden().to(device)\n","\n","    predicted_tag_id = None\n","    # Your code here!\n","\n","    return predicted_tag_id\n","\n","\n","rnn_model.eval()\n","predicted_tags = []\n","for sent_tensor in test_data_oh_list:\n","    sent_tensor = sent_tensor.to(device)\n","    predicted_tag_id = rnn_predict_one_sent(rnn_model, sent_tensor)\n","    predicted_tags.append([id2tag[idx] for idx in predicted_tag_id.detach().cpu().numpy()])\n","\n","# Will output precision, recall, and f1 score.\n","evaluate_result(test_tags, predicted_tags)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y3uM___pcp0j","colab_type":"text"},"source":["## **Part 1: Lab writeup**\n","\n","Part 1 of your lab report should discuss any implementation details that were important to filling out the code above. Then, use the code to set up experiments that answer the following questions:\n","\n","1. Compute the gradient norm of all trainable parameters along the training. Plot a figure similar to what we did for training loss (x-axis is training iterations and y-axis is avg. gradient norm value). Describe what you find. Hint: gradient norm is the L2 norm of of the gradient vector.\n","\n","2. Try to do learning with gradient clipping. Why should we use gradient clipping? What is the effect of gradient clipping? Could we use larger learning rate after gradient clipping? Why? Hint: use torch.nn.utils.clip_grad_norm(model.parameters(), clip) after loss.backward(). You can try various clipping range. A good starting point is clip = 5."]},{"cell_type":"markdown","metadata":{"id":"EuutnMvlxltt","colab_type":"text"},"source":["# **Part 2: Long Short-Term Memory (LSTM)**\n","\n","In part 2, you will implement your own LSTM. Same in part 1, you are not allowed to use PyTorch built-in LSTM/LSTMCell modules. For reference, you can look up the wikipage for the [LSTM architecture](https://en.wikipedia.org/wiki/Long_short-term_memory)."]},{"cell_type":"code","metadata":{"id":"LTBM91nqfkA4","colab_type":"code","colab":{}},"source":["class LSTM(nn.Module):\n","    # A torch module implementing an LSTM. The `forward` function should just\n","    # perform one step of update and output logits before softmax.\n","\n","    def __init__(self, input_size, hidden_size, output_size):\n","        super(LSTM, self).__init__()\n","\n","        # Your code here!\n","\n","\n","    def forward(self, input, hidden, memory):\n","        # `input` is a 2d-tensor of shape (1, input_size);\n","        # `hidden` and `memory` are both 2d-tensors of shape (1, hidden_size),\n","        # representing the hidden and memory states of the previous time step.\n","\n","        output = None\n","        memory = None\n","        # Your code here!\n","\n","        return output, hidden, memory\n","\n","    def initHidden(self):\n","        # Initialize hidden and memory states.\n","        return (torch.zeros(1, self.hidden_size),\n","                torch.zeros(1, self.hidden_size))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ed2Mt5o5hoqh","colab_type":"text"},"source":["Same with part 1, we provide the main training loop for training LSTM, but you will need to implement the `lstm_train_one_sample` function, and also apply gradient clipping."]},{"cell_type":"code","metadata":{"id":"sNn7sBQffs-4","colab_type":"code","colab":{}},"source":["learning_rate = 1e-3\n","lstm_hidden_size = 128\n","\n","lstm_model = LSTM(input_size=num_vocabs, hidden_size=lstm_hidden_size,\n","                  output_size=num_tags).to(device)\n","\n","criterion = nn.CrossEntropyLoss()\n","lstm_optimizer = torch.optim.Adam(lstm_model.parameters(),  lr=learning_rate)\n","\n","\n","def lstm_train_one_sample(model, sent_tensor, tag_tensor):\n","    hidden, memory = model.initHidden()\n","    hidden = hidden.to(device)\n","    memory = memory.to(device)\n","    # Your code here!\n","\n","\n","    loss = criterion(outputs, tag_tensor)\n","\n","    lstm_optimizer.zero_grad()\n","    loss.backward()\n","\n","    # Your Code here!\n","\n","    \n","    lstm_optimizer.step()\n","\n","    return outputs, loss.item()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WRN8XcIojx6g","colab_type":"text"},"source":["LSTM main training loop:"]},{"cell_type":"code","metadata":{"id":"KKzhbwW0q5to","colab_type":"code","colab":{}},"source":["n_epochs = 5\n","iter_count = 0\n","print_every = 1000\n","plot_every = 50\n","\n","# Keep track of losses for plotting\n","current_loss = 0\n","all_losses = []\n","\n","start = time.time()\n","\n","lstm_model.train()\n","for epoch_i in range(n_epochs):\n","    for sent_tensor, tag_tensor in zip(train_data_oh_list, train_tags_id_list):\n","        sent_tensor = sent_tensor.to(device)\n","        tag_tensor = tag_tensor.to(device)\n","  \n","        output, loss = lstm_train_one_sample(lstm_model, sent_tensor, tag_tensor)\n","        current_loss += loss\n","\n","        if iter_count % print_every == 0:\n","            print('%d %s %.4f' % (iter_count, timeSince(start), loss))\n","\n","        # Add current loss avg to list of losses\n","        if iter_count % plot_every == 0 and iter_count > 0:\n","            all_losses.append(current_loss / plot_every)\n","            current_loss = 0\n","\n","        iter_count += 1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4b_iAmgztI-T","colab_type":"code","colab":{}},"source":["plt.figure()\n","plt.plot(all_losses)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"52ebi-K2k1zq","colab_type":"text"},"source":["Now let's evaluate our LSTM model."]},{"cell_type":"code","metadata":{"id":"ncDGLjsMluAe","colab_type":"code","colab":{}},"source":["def lstm_predict_one_sent(model, sent_tensor):\n","    hidden, memory = model.initHidden()\n","    hidden = hidden.to(device)\n","    memory = memory.to(device)\n","    # Your code here!\n","\n","    return predicted_tag_id\n","\n","\n","lstm_model.eval()\n","predicted_tags = []\n","for sent_tensor in test_data_oh_list:\n","    sent_tensor = sent_tensor.to(device)\n","    predicted_tag_id = lstm_predict_one_sent(lstm_model, sent_tensor)\n","    predicted_tags.append([id2tag[idx] for idx in predicted_tag_id.detach().cpu().numpy()])\n","\n","# Will output precision, recall, and f1 score.\n","evaluate_result(test_tags, predicted_tags)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rHltRZH4gfI2","colab_type":"text"},"source":["## **Part 2: Lab writeup**\n","\n","Your lab report should discuss any implementation details that were important to filling out the code above. Then, use the code to set up experiments that answer the following questions:\n","\n","1. Change lstm_hidden_size so that LSTM has approximately the same amount of parameters as RNN. Hint: use sum(p.numel() for p in model.parameters() if p.requires_grad) to find the number of trainable parameters. Re-train the LSTM model and discuss your findings.\n","\n","2. Same in part 1 writeup, plot the gradient norm figure for LSTM. Discuss your findings.\n","\n","3. Currently we update model parameters for each sample (1 sample = 1 iteration). In practice, we usually train with a mini-batch (mB): the entire dataset of N samples is split into N/mB batches, and use one mini-batch for each update. Briefly describe how to do batching in our dataset. Hint: each sentence can have variable length."]}]}