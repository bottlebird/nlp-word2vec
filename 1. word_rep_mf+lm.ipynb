{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"1. word_rep_mf+lm.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"cells":[{"cell_type":"code","metadata":{"colab_type":"code","id":"FU7xWiY6TyWS","outputId":"7f60ebbc-7c55-4dae-cd0d-4655069b96f9","executionInfo":{"status":"ok","timestamp":1586854215206,"user_tz":240,"elapsed":1029,"user":{"displayName":"ByeongJo Kong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjB3OJt2FV1VBgbxRHjhh6mK4NBkVW9ONJVSMVJ=s64","userId":"10931837966081205326"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%%bash\n","!(stat -t /usr/local/lib/*/dist-packages/google/colab > /dev/null 2>&1) && exit \n","rm -rf 6864-hw1\n","git clone https://github.com/lingo-mit/6864-hw1.git"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Cloning into '6864-hw1'...\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"A0MHaHrdUACZ","colab":{}},"source":["import sys\n","sys.path.append(\"/content/6864-hw1\")\n","\n","import csv\n","import itertools as it\n","import numpy as np\n","np.random.seed(0)\n","\n","import lab_util"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"cZ3MUj4iUf76"},"source":["## Introduction\n","\n","This lab will explore three different ways of using unlabeled text data to learn pretrained word representations. It will describe the effects of different modeling decisions (representation learning objective, context size, etc.) on both qualitative properties of learned representations and their effect on a downstream prediction problem."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"gG654Y9J3yHw"},"source":["\n","The lab will work with a dataset of product reviews. It looks like this:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"JwiX-Tc9V1xI","scrolled":true,"outputId":"b287227a-4251-41e1-8a2f-2682bd82b568","executionInfo":{"status":"ok","timestamp":1586854219009,"user_tz":240,"elapsed":423,"user":{"displayName":"ByeongJo Kong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjB3OJt2FV1VBgbxRHjhh6mK4NBkVW9ONJVSMVJ=s64","userId":"10931837966081205326"}},"colab":{"base_uri":"https://localhost:8080/","height":360}},"source":["data = []\n","n_positive = 0\n","n_disp = 0\n","with open(\"/content/6864-hw1/reviews.csv\") as reader:\n","  csvreader = csv.reader(reader)\n","  next(csvreader)\n","  for id, review, label in csvreader:\n","    label = int(label)\n","\n","    # hacky class balancing\n","    if label == 1:\n","      if n_positive == 2000:\n","        continue\n","      n_positive += 1\n","    if len(data) == 4000:\n","      break\n","\n","    data.append((review, label))\n","    \n","    if n_disp > 5:\n","      continue\n","    n_disp += 1\n","    print(\"review:\", review)\n","    print(\"rating:\", label, \"(good)\" if label == 1 else \"(bad)\")\n","    print()\n","\n","print(f\"Read {len(data)} total reviews.\")\n","np.random.shuffle(data)\n","reviews, labels = zip(*data)\n","train_reviews = reviews[:3000]\n","train_labels = labels[:3000]\n","val_reviews = reviews[3000:3500]\n","val_labels = labels[3000:3500]\n","test_reviews = reviews[3500:]\n","test_labels = labels[3500:]"],"execution_count":0,"outputs":[{"output_type":"stream","text":["review: I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.\n","rating: 1 (good)\n","\n","review: Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product as \"Jumbo\".\n","rating: 0 (bad)\n","\n","review: This is a confection that has been around a few centuries.  It is a light, pillowy citrus gelatin with nuts - in this case Filberts. And it is cut into tiny squares and then liberally coated with powdered sugar.  And it is a tiny mouthful of heaven.  Not too chewy, and very flavorful.  I highly recommend this yummy treat.  If you are familiar with the story of C.S. Lewis' \"The Lion, The Witch, and The Wardrobe\" - this is the treat that seduces Edmund into selling out his Brother and Sisters to the Witch.\n","rating: 1 (good)\n","\n","review: If you are looking for the secret ingredient in Robitussin I believe I have found it.  I got this in addition to the Root Beer Extract I ordered (which was good) and made some cherry soda.  The flavor is very medicinal.\n","rating: 0 (bad)\n","\n","review: Great taffy at a great price.  There was a wide assortment of yummy taffy.  Delivery was very quick.  If your a taffy lover, this is a deal.\n","rating: 1 (good)\n","\n","review: I got a wild hair for taffy and ordered this five pound bag. The taffy was all very enjoyable with many flavors: watermelon, root beer, melon, peppermint, grape, etc. My only complaint is there was a bit too much red/black licorice-flavored pieces (just not my particular favorites). Between me, my kids, and my husband, this lasted only two weeks! I would recommend this brand of taffy -- it was a delightful treat.\n","rating: 1 (good)\n","\n","Read 4000 total reviews.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"twLHWqM6Z5xD"},"source":["## Part 1: word representations via matrix factorization\n","\n","First, the term--document matrix is constructed (look at `lab_util.py` in the file browser to see how this works)."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"3WPt6Y7-Z_7P","outputId":"dfa75404-086c-493b-8036-46a7bb4706a6","executionInfo":{"status":"ok","timestamp":1586854222060,"user_tz":240,"elapsed":1179,"user":{"displayName":"ByeongJo Kong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjB3OJt2FV1VBgbxRHjhh6mK4NBkVW9ONJVSMVJ=s64","userId":"10931837966081205326"}},"colab":{"base_uri":"https://localhost:8080/","height":153}},"source":["vectorizer = lab_util.CountVectorizer()\n","vectorizer.fit(train_reviews)\n","td_matrix = vectorizer.transform(train_reviews).T\n","print(f\"TD matrix is {td_matrix.shape[0]} x {td_matrix.shape[1]}\")\n","\n","print(td_matrix)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["TD matrix is 2006 x 3000\n","[[3. 3. 5. ... 1. 6. 4.]\n"," [1. 0. 0. ... 0. 0. 0.]\n"," [2. 0. 0. ... 0. 0. 0.]\n"," ...\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"KASVs8KubeBE","colab":{}},"source":["from sklearn.decomposition import TruncatedSVD\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","def learn_reps_lsa(matrix, rep_size):\n","  # `matrix` is a `|V| x n` matrix, where `|V|` is the number of words in the\n","  # vocabulary. This function should return a `|V| x rep_size` matrix with each\n","  # row corresponding to a word representation. The `sklearn.decomposition` \n","  # package may be useful.\n","\n","    svd_model = TruncatedSVD(n_components=rep_size)\n","    svd_model.fit(matrix)\n","    svd_matrix=svd_model.transform(matrix)\n","    \n","    return svd_matrix"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"SKWzRC0dclVK"},"source":["Let's look at some representations:"]},{"cell_type":"code","metadata":{"id":"2FAAhXhd4u8g","colab_type":"code","outputId":"872370aa-127f-4953-d643-f956dd7b0b79","executionInfo":{"status":"ok","timestamp":1586854304471,"user_tz":240,"elapsed":734,"user":{"displayName":"ByeongJo Kong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjB3OJt2FV1VBgbxRHjhh6mK4NBkVW9ONJVSMVJ=s64","userId":"10931837966081205326"}},"colab":{"base_uri":"https://localhost:8080/","height":731}},"source":["reps = learn_reps_lsa(td_matrix, 64)\n","words = [\"good\", \"bad\", \"cookie\", \"jelly\", \"dog\", \"the\", \"4\"]\n","show_tokens = [vectorizer.tokenizer.word_to_token[word] for word in words]\n","lab_util.show_similar_words(vectorizer.tokenizer, reps, show_tokens)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["good 47\n","  pretty 0.670\n","  liked 0.782\n","  everyone 0.811\n","  really 0.817\n","  beat 0.865\n","bad 201\n","  taste 0.595\n","  really 0.614\n","  ok 0.623\n","  . 0.638\n","  didn't 0.654\n","cookie 504\n","  nana's 0.471\n","  cookies 0.601\n","  gluten 0.816\n","  free 0.838\n","  shortbread 0.844\n","jelly 351\n","  bread 1.039\n","  online 1.043\n","  low 1.043\n","  hoping 1.094\n","  twist 1.102\n","dog 925\n","  pet 0.264\n","  dogs 0.308\n","  food 0.385\n","  nutritious 0.433\n","  pets 0.475\n","the 36\n","  . 0.331\n","  <unk> 0.366\n","  of 0.394\n","  and 0.402\n","  to 0.422\n","4 292\n","  1 0.206\n","  6 0.210\n","  2 0.319\n","  70 0.434\n","  5 0.458\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"LsOAGLB3iRjT"},"source":["Here, the TF-IDF transform is implemented."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"1y3PmW-IgpqA","colab":{}},"source":["import math\n","\n","def transform_tfidf(matrix):\n","  # `matrix` is a `|V| x |D|` matrix of raw counts, where `|V|` is the \n","  # vocabulary size and `|D|` is the number of documents in the corpus. This\n","  # function should (nondestructively) return a version of `matrix` with the\n","  # TF-IDF transform appliied.\n","\n","  thresholded = matrix > 1\n","  dfs = thresholded.sum(axis=1)[:, np.newaxis]\n","  idfs = np.log(matrix.shape[1]) - np.log(dfs + 1e-8)\n","\n","  return matrix * idfs"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"xOprgqzHi7bk"},"source":["Let's see how does this change the learned similarity function."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"SV5xKLYTi7LA","scrolled":true,"outputId":"8ecbefc8-64fb-4623-c37a-0853a712dd36","executionInfo":{"status":"ok","timestamp":1586854450416,"user_tz":240,"elapsed":750,"user":{"displayName":"ByeongJo Kong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjB3OJt2FV1VBgbxRHjhh6mK4NBkVW9ONJVSMVJ=s64","userId":"10931837966081205326"}},"colab":{"base_uri":"https://localhost:8080/","height":731}},"source":["td_matrix_tfidf = transform_tfidf(td_matrix)\n","#reps = learn_reps_lsa(td_matrix, 1000)  #SVD만 적용 bigger the better\n","reps_tfidf = learn_reps_lsa(td_matrix_tfidf, 64)  #TFIDF로 Term-doc 관련성/중요도 적용 후 SVD 적용\n","#w_co = learn_reps_lsa(w_tt, 1000)  #TFIDF로 Term-doc 관련성/중요도 적용 후 SVD 적용\n","lab_util.show_similar_words(vectorizer.tokenizer, reps_tfidf, show_tokens)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["good 47\n","  but 0.326\n","  . 0.401\n","  like 0.413\n","  is 0.430\n","  too 0.433\n","bad 201\n","  taste 0.391\n","  like 0.451\n","  just 0.511\n","  me 0.514\n","  but 0.521\n","cookie 504\n","  cookies 0.658\n","  nana's 0.697\n","  gluten 0.744\n","  flour 0.821\n","  free 0.830\n","jelly 351\n","  mixing 0.809\n","  advertised 0.963\n","  save 0.963\n","  muffins 0.976\n","  vanilla 0.986\n","dog 925\n","  pet 0.362\n","  dogs 0.404\n","  food 0.468\n","  pets 0.544\n","  switched 0.554\n","the 36\n","  . 0.086\n","  of 0.103\n","  to 0.122\n","  and 0.132\n","  in 0.157\n","4 292\n","  6 0.178\n","  1 0.206\n","  2 0.468\n","  70 0.506\n","  5 0.601\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"HO-NG4u1kG9z"},"source":["Now that we have some representations, let's see if we can do something useful with them.\n","\n","Below is the implementation of a feature function that represents a document as the sum of its\n","learned word embeddings.\n","\n","The remaining code trains a logistic regression model on a set of *labeled* reviews; we're interested in seeing how much representations learned from *unlabeled* reviews improve classification."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"6B08xvIFlee3","outputId":"da0e723d-167c-4005-c125-5d95a2020fad","executionInfo":{"status":"ok","timestamp":1586854585713,"user_tz":240,"elapsed":4845,"user":{"displayName":"ByeongJo Kong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjB3OJt2FV1VBgbxRHjhh6mK4NBkVW9ONJVSMVJ=s64","userId":"10931837966081205326"}},"colab":{"base_uri":"https://localhost:8080/","height":476}},"source":["REP_DICT = learn_reps_lsa(td_matrix_tfidf, 64)\n","\n","def word_featurizer(xs):\n","  # normalize\n","  return xs / np.sqrt((xs ** 2).sum(axis=1, keepdims=True))\n","\n","def lsa_featurizer(xs):\n","  # This function takes in a matrix in which each row contains the word counts\n","  # for the given review. It should return a matrix in which each row contains\n","  # the learned feature representation of each review (e.g. the sum of LSA \n","  # word representations).\n","\n","  feats = sum(np.outer(xs[:, i], REP_DICT[i, :]) for i in range(xs.shape[1]))\n","  # normalize\n","  return feats / np.sqrt((feats ** 2).sum(axis=1, keepdims=True))\n","\n","def combo_featurizer(xs):\n","  return np.concatenate((word_featurizer(xs), lsa_featurizer(xs)), axis=1)\n","\n","def train_model(featurizer, xs, ys):\n","  import sklearn.linear_model\n","  xs_featurized = featurizer(xs)\n","  model = sklearn.linear_model.LogisticRegression()\n","  model.fit(xs_featurized, ys)\n","  return model\n","\n","def eval_model(model, featurizer, xs, ys):\n","  xs_featurized = featurizer(xs)\n","  pred_ys = model.predict(xs_featurized)\n","  print(\"test accuracy\", np.mean(pred_ys == ys))\n","\n","def training_experiment(name, featurizer, n_train):\n","  print(f\"{name} features, {n_train} examples\")\n","  train_rv = vectorizer.transform(train_reviews[:n_train])\n","  train_lb = train_labels[:n_train]\n","  test_rv = vectorizer.transform(test_reviews)\n","  test_lb = test_labels\n","  model = train_model(featurizer, train_rv, train_lb)\n","  eval_model(model, featurizer, test_rv, test_lb)\n","  print()\n","\n","training_experiment(\"word\", word_featurizer, 20)\n","training_experiment(\"lsa\", lsa_featurizer, 20)\n","training_experiment(\"combo\", combo_featurizer, 20)\n","\n","training_experiment(\"word\", word_featurizer, 100)\n","training_experiment(\"lsa\", lsa_featurizer, 100)\n","training_experiment(\"combo\", combo_featurizer, 100)\n","\n","training_experiment(\"word\", word_featurizer, 1000)\n","training_experiment(\"lsa\", lsa_featurizer, 1000)\n","training_experiment(\"combo\", combo_featurizer, 1000)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["word features, 20 examples\n","test accuracy 0.526\n","\n","lsa features, 20 examples\n","test accuracy 0.524\n","\n","combo features, 20 examples\n","test accuracy 0.526\n","\n","word features, 100 examples\n","test accuracy 0.616\n","\n","lsa features, 100 examples\n","test accuracy 0.6\n","\n","combo features, 100 examples\n","test accuracy 0.624\n","\n","word features, 1000 examples\n","test accuracy 0.784\n","\n","lsa features, 1000 examples\n","test accuracy 0.682\n","\n","combo features, 1000 examples\n","test accuracy 0.784\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"AxfunCYh5nmZ"},"source":["## Part 2: word representations via language modeling\n","\n","This section will train a word embedding model with a word2vec-style objective rather than a matrix factorization objective."]},{"cell_type":"code","metadata":{"id":"ZflH0gCE2LC7","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torch.utils.data as torch_data\n","from torch.autograd import Variable\n","\n","class Word2VecModel(nn.Module):\n","  # A torch module implementing a word2vec predictor. The `forward` function\n","  # should take a batch of context word ids as input and predict the word \n","  # in the middle of the context as output, as in the CBOW model from lecture.\n","\n","  def __init__(self, vocab_size, embed_dim):\n","        # Your code here!\n","        super().__init__()\n","        self.embeds = nn.Embedding(vocab_size, embed_dim)\n","        self.linear1 = nn.Linear(embed_dim, 64)\n","        self.linear2 = nn.Linear(64, vocab_size)\n","      \n","  def forward(self, context):\n","      # Context is an `n_batch x n_context` matrix of integer word ids\n","      # this function should return a set of scores for predicting the word \n","      # in the middle of the context\n","        # Your code here!\n","        \n","        output = self.embeds(context) # get the embeddings\n","        output = output.sum(dim=1)\n","        output = F.relu(self.linear1(output)) # pass through first layer\n","        output = self.linear2(output) # pass through second layer\n","\n","        return output"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ePgZlityuWr3","colab":{}},"source":["import time\n","\n","def learn_reps_word2vec(corpus, window_size, rep_size, n_epochs, n_batch):\n","  # This method takes in a corpus of training sentences. It returns a matrix of\n","  # word embeddings with the same structure as used in the previous section of \n","  # the assignment. (You can extract this matrix from the parameters of the \n","  # Word2VecModel.)\n","\n","  tokenizer = lab_util.Tokenizer()\n","  tokenizer.fit(corpus)\n","  tokenized_corpus = tokenizer.tokenize(corpus)\n","  #(array([48,  0,  8, 44]), 49),\n","  #(array([ 0, 49, 44,  3]), 8),   ngrams format: (context words , target word)\n","  ngrams = lab_util.get_ngrams(tokenized_corpus, window_size) #context matrix\n","\n","  device = torch.device('cuda')  # run on colab gpu\n","  model = Word2VecModel(tokenizer.vocab_size, rep_size).to(device)\n","  opt = optim.Adam(model.parameters(), lr=0.001) \n","  loss_fn = nn.CrossEntropyLoss()\n","\n","  loader = torch_data.DataLoader(ngrams, batch_size=n_batch, shuffle=True)\n","\n","  start = time.time()\n","  for epoch in range(n_epochs):\n","    epoch_loss = 0\n","    n_batches = 0\n","    #total_loss = 0\n","    for context, label in loader:\n","      # as described above, `context` is a batch of context word ids, and\n","      # `label` is a batch of predicted word labels\n","      #context_var = torch.grad.Variable(torch.LongTensor(context))\n","      preds = model(context.to(device))\n","      loss=loss_fn(preds, label.to(device))\n","      \n","      opt.zero_grad()\n","      loss.backward()\n","      opt.step()\n","\n","      epoch_loss += loss.item()\n","      n_batches += 1\n","\n","    epoch_loss /= n_batches\n","    if (epoch+1)%10 ==0:\n","      print(f\"epoch {epoch+1}: {epoch_loss}, {time.time()-start}s\")\n","      start = time.time()\n","\n","  # reminder: you want to return a `vocab_size x embedding_size` numpy array\n","  embedding_matrix = next(model.embeds.parameters())\n","  return embedding_matrix.cpu().detach().numpy()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fd2JpYxy2LDH","colab_type":"code","outputId":"a1e58053-de04-4776-98f1-fb86b2a485c1","executionInfo":{"status":"ok","timestamp":1586855958146,"user_tz":240,"elapsed":552617,"user":{"displayName":"ByeongJo Kong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjB3OJt2FV1VBgbxRHjhh6mK4NBkVW9ONJVSMVJ=s64","userId":"10931837966081205326"}},"colab":{"base_uri":"https://localhost:8080/","height":357}},"source":["reps_word2vec = learn_reps_word2vec(train_reviews, 1, 32, 200, 1024)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["epoch 10: 4.260442705190137, 27.9047908782959s\n","epoch 20: 3.991017733620347, 27.69009256362915s\n","epoch 30: 3.8554277089651157, 27.87818193435669s\n","epoch 40: 3.770531847235862, 27.725236415863037s\n","epoch 50: 3.7098305814721613, 27.70301628112793s\n","epoch 60: 3.6639440443631863, 27.525153875350952s\n","epoch 70: 3.628050940760066, 27.614866018295288s\n","epoch 80: 3.598982144830825, 27.57706379890442s\n","epoch 90: 3.5748120631171525, 27.48516273498535s\n","epoch 100: 3.55465012453915, 27.574446439743042s\n","epoch 110: 3.537092481213116, 27.471062660217285s\n","epoch 120: 3.5218868746739647, 27.32981514930725s\n","epoch 130: 3.5082695037684637, 27.559032678604126s\n","epoch 140: 3.4957497602098444, 27.448981046676636s\n","epoch 150: 3.4851980548672907, 27.528251886367798s\n","epoch 160: 3.475864617565598, 27.421628952026367s\n","epoch 170: 3.4667931335249196, 27.49956250190735s\n","epoch 180: 3.459347671337342, 27.48327136039734s\n","epoch 190: 3.4517817997307367, 27.20704436302185s\n","epoch 200: 3.4457431411028803, 27.177741527557373s\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"O3oE-tpR7I39"},"source":["After training the embeddings, we can try to visualize the embedding space to see if it makes sense. First, we can take any word in the space and check its closest neighbors."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"yMW4QND56bHF","outputId":"cc15477f-6b90-4383-a36e-5ed6187efe83","executionInfo":{"status":"ok","timestamp":1586856267293,"user_tz":240,"elapsed":508,"user":{"displayName":"ByeongJo Kong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjB3OJt2FV1VBgbxRHjhh6mK4NBkVW9ONJVSMVJ=s64","userId":"10931837966081205326"}},"colab":{"base_uri":"https://localhost:8080/","height":731}},"source":["lab_util.show_similar_words(vectorizer.tokenizer, reps_word2vec, show_tokens)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["good 47\n","  fine 0.529\n","  bad 0.559\n","  nice 0.596\n","  decent 0.627\n","  tasty 0.716\n","bad 201\n","  good 0.559\n","  easy 0.665\n","  weird 0.809\n","  strong 0.811\n","  greatest 0.826\n","cookie 504\n","  sized 0.827\n","  fat 0.895\n","  version 1.000\n","  yum 1.025\n","  meals 1.030\n","jelly 351\n","  stock 0.848\n","  run 1.014\n","  loose 1.035\n","  chowder 1.055\n","  spot 1.097\n","dog 925\n","  baby 0.775\n","  cat 0.792\n","  junk 0.814\n","  old 0.918\n","  pet 0.938\n","the 36\n","  their 0.679\n","  my 0.810\n","  your 0.841\n","  our 0.967\n","  any 0.990\n","4 292\n","  2 0.485\n","  5 0.522\n","  25 0.727\n","  3 0.731\n","  15 0.773\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ue-9CPSc7fi9"},"source":["We can also cluster the embedding space. Clustering in 4 or more dimensions is hard to visualize, and even clustering in 2 or 3 can be difficult because there are so many words in the vocabulary. One thing we can try to do is assign cluster labels and qualitiatively look for an underlying pattern in the clusters."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"v-Yf6NMCXVx4","outputId":"e54e7715-0551-469e-d443-2c9125f69aab","executionInfo":{"status":"ok","timestamp":1586856271107,"user_tz":240,"elapsed":1181,"user":{"displayName":"ByeongJo Kong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjB3OJt2FV1VBgbxRHjhh6mK4NBkVW9ONJVSMVJ=s64","userId":"10931837966081205326"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["from sklearn.cluster import KMeans\n","\n","indices = KMeans(n_clusters=10).fit_predict(reps_word2vec)\n","zipped = list(zip(range(vectorizer.tokenizer.vocab_size), indices))\n","np.random.shuffle(zipped)\n","zipped = zipped[:100]\n","zipped = sorted(zipped, key=lambda x: x[1])\n","for token, cluster_idx in zipped:\n","  word = vectorizer.tokenizer.token_to_word[token]\n","  print(f\"{word}: {cluster_idx}\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["junk: 0\n","owner: 0\n","greta: 0\n","preference: 0\n","barbecue: 0\n","switch: 0\n","manufacturer: 0\n","mccann's: 0\n","his: 0\n","tin: 0\n","pain: 0\n","suspect: 0\n","chicken: 0\n","lobster: 0\n","kinds: 0\n","record: 0\n","soup: 0\n","negative: 1\n","total: 1\n","tells: 1\n","bone: 1\n","source: 1\n","deal: 1\n","snack: 1\n","many: 1\n","larger: 1\n","suggested: 2\n","eating: 2\n","molasses: 2\n","began: 2\n","how: 2\n","purchasing: 2\n","needed: 2\n","wanting: 2\n","lost: 2\n","stock: 2\n","left: 2\n","off: 2\n","bed: 3\n","high: 3\n","economical: 3\n","decent: 3\n","missing: 3\n","order: 4\n","avoid: 4\n","thinking: 4\n","went: 4\n","seemed: 4\n","teaspoon: 4\n","dip: 4\n","chocolates: 5\n","possible: 5\n","nectar: 5\n","lemonade: 5\n","oven: 5\n","ago: 5\n","reviews: 5\n","bodied: 6\n","restaurant: 6\n","orange: 6\n","kibble: 6\n","balance: 6\n","salty: 6\n","pasta: 6\n","acid: 6\n","oils: 6\n",".: 7\n","hoping: 7\n","otherwise: 7\n","wow: 7\n","else: 7\n","i've: 7\n","they're: 7\n","i'll: 7\n","since: 7\n","yeah: 7\n","china: 7\n","purina: 8\n","benefit: 8\n","home: 8\n","multiple: 8\n","from: 8\n","figured: 8\n","walmart: 8\n","gritty: 8\n","sure: 8\n","u: 9\n","serving: 9\n","breakfast: 9\n","favor: 9\n","minute: 9\n","wheat: 9\n","value: 9\n","homemade: 9\n","above: 9\n","fried: 9\n","person: 9\n","hands: 9\n","salads: 9\n","maple: 9\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ci1TkENU78Wn"},"source":["Finally, we can use the trained word embeddings to construct vector representations of full reviews. One common approach is to simply average all the word embeddings in the review to create an overall embedding."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"A5vjmRV6Dgbu","outputId":"d6d93a4e-7d52-4a6d-8b87-8aa4c2b50fa0","executionInfo":{"status":"ok","timestamp":1586856301448,"user_tz":240,"elapsed":568,"user":{"displayName":"ByeongJo Kong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjB3OJt2FV1VBgbxRHjhh6mK4NBkVW9ONJVSMVJ=s64","userId":"10931837966081205326"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["def lsa_featurizer(xs):\n","  feats = sum(np.outer(xs[:, i], reps_word2vec[i, :]) for i in range(xs.shape[1]))\n","\n","  # normalize\n","  return feats / np.sqrt((feats ** 2).sum(axis=1, keepdims=True))\n","\n","training_experiment(\"word2vec\", lsa_featurizer, 10)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["word2vec features, 10 examples\n","test accuracy 0.474\n","\n"],"name":"stdout"}]}]}